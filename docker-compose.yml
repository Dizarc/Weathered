services:
  weathered:
    build:
      context: .
      dockerfile: weathered.dockerfile

    container_name: weathered-app
    restart: unless-stopped
    
    devices:
      - "/dev/dri:/dev/dri" # GPU access
      
    environment:
      - QT_QPA_PLATFORM=eglfs
      - API_KEY=${API_KEY}
      - API_CITY_COUNTRY=${API_CITY_COUNTRY}
      - LM_URL=http://llama-server:8080/v1/chat/completions

    networks:
      - weathered-net
    
  llama-server:
    build:
      context: .
      dockerfile: llama.dockerfile
    
    llama-server:
    image: ghcr.io/ggml-org/llama.cpp:server
    
    container_name: llama-server-api
    restart: unless-stopped

    ports:
      - "8080:8080"

    volumes:
      - ${MODEL_PATH}:/models/model.gguf

    command: ["-m", "/models/model.gguf", "--host", "0.0.0.0", "--port", "8080", "--n-gpu-layers", "0"]

    networks:
      - weathered-net

networks:
  weathered-net:
    driver: bridge
